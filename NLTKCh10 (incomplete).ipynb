{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10   Analyzing the Meaning of Sentences\n",
    "\n",
    "We have seen how useful it is to harness the power of a computer to process text on a large scale. However, now that we have the machinery of parsers and feature based grammars, can we do anything similarly useful by analyzing the meaning of sentences? The goal of this chapter is to answer the following questions:\n",
    "\n",
    "* How can we represent natural language meaning so that a computer can process these representations?\n",
    "* How can we associate meaning representations with an unlimited set of sentences?\n",
    "* How can we use programs that connect the meaning representations of sentences to stores of knowledge?\n",
    "\n",
    "Along the way we will learn some formal techniques in the field of logical semantics, and see how these can be used for interrogating databases that store facts about the world.\n",
    "\n",
    "## 10.1   Natural Language Understanding\n",
    "\n",
    "### Querying a Database\n",
    "\n",
    "Suppose we have a program that lets us type in a natural language question and gives us back the right answer:\n",
    "\n",
    "(1)\t\t\n",
    "\n",
    "a.\t\tWhich country is Athens in?\n",
    "\n",
    "b.\t\tGreece.\n",
    "\n",
    "How hard is it to write such a program? And can we just use the same techniques that we've encountered so far in this book, or does it involve something new? In this section, we will show that solving the task in a restricted domain is pretty straightforward. But we will also see that to address the problem in a more general way, we have to open up a whole new box of ideas and techniques, involving the representation of meaning.\n",
    "\n",
    "So let's start off by assuming that we have data about cities and countries in a structured form. To be concrete, we will use a database table whose first few rows are shown in 10.1.\n",
    "\n",
    "Note\n",
    "\n",
    "The data illustrated in 10.1 is drawn from the Chat-80 system (Warren & Pereira, 1982). Population figures are given in thousands, but note that the data used in these examples dates back at least to the 1980s, and was already somewhat out of date at the point when (Warren & Pereira, 1982) was published.\n",
    "\n",
    "Table 10.1:\n",
    "\n",
    "city_table: A table of cities, countries and populations\n",
    "\n",
    "City    Country Population\n",
    "\n",
    "athens  greece \t1368\n",
    "\n",
    "bangkok   \tthailand \t1178\n",
    "\n",
    "barcelona   \tspain \t1280\n",
    "\n",
    "berlin      east_germany \t3481\n",
    "\n",
    "birmingham    united_kingdom \t1112\n",
    "\n",
    "The obvious way to retrieve answers from this tabular data involves writing queries in a database query language such as SQL.\n",
    "\n",
    "Note: SQL (Structured Query Language) is a language designed for retrieving and managing data in relational databases. If you want to find out more about SQL, http://www.w3schools.com/sql/ is a convenient online reference.\n",
    "\n",
    "For example, executing the query (2) will pull out the value 'greece':\n",
    "\n",
    "(2)\t\tSELECT Country FROM city_table WHERE City = 'athens'\n",
    "\n",
    "This specifies a result set consisting of all values for the column Country in data rows where the value of the City column is 'athens'.\n",
    "\n",
    "How can we get the same effect using English as our input to the query system? The feature-based grammar formalism described in 9 makes it easy to translate from English to SQL. The grammar sql0.fcfg illustrates how to assemble a meaning representation for a sentence in tandem with parsing the sentence. Each phrase structure rule is supplemented with a recipe for constructing a value for the feature sem. You can see that these recipes are extremely simple; in each case, we use the string concatenation operation + to splice the values for the child constituents to make a value for the parent constituent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "S[SEM=(?np + WHERE + ?vp)] -> NP[SEM=?np] VP[SEM=?vp]\n",
      "VP[SEM=(?v + ?pp)] -> IV[SEM=?v] PP[SEM=?pp]\n",
      "VP[SEM=(?v + ?ap)] -> IV[SEM=?v] AP[SEM=?ap]\n",
      "NP[SEM=(?det + ?n)] -> Det[SEM=?det] N[SEM=?n]\n",
      "PP[SEM=(?p + ?np)] -> P[SEM=?p] NP[SEM=?np]\n",
      "AP[SEM=?pp] -> A[SEM=?a] PP[SEM=?pp]\n",
      "NP[SEM='Country=\"greece\"'] -> 'Greece'\n",
      "NP[SEM='Country=\"china\"'] -> 'China'\n",
      "Det[SEM='SELECT'] -> 'Which' | 'What'\n",
      "N[SEM='City FROM city_table'] -> 'cities'\n",
      "IV[SEM=''] -> 'are'\n",
      "A[SEM=''] -> 'located'\n",
      "P[SEM=''] -> 'in'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('grammars/book_grammars/sql0.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to parse a query into SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.W.c.a.l.i.C.|\n",
      "Leaf Init Rule:\n",
      "|[-] . . . . .| [0:1] 'What'\n",
      "|. [-] . . . .| [1:2] 'cities'\n",
      "|. . [-] . . .| [2:3] 'are'\n",
      "|. . . [-] . .| [3:4] 'located'\n",
      "|. . . . [-] .| [4:5] 'in'\n",
      "|. . . . . [-]| [5:6] 'China'\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[-] . . . . .| [0:1] Det[SEM='SELECT'] -> 'What' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[-> . . . . .| [0:1] NP[SEM=(?det+?n)] -> Det[SEM=?det] * N[SEM=?n] {?det: 'SELECT'}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. [-] . . . .| [1:2] N[SEM='City FROM city_table'] -> 'cities' *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[---] . . . .| [0:2] NP[SEM=(SELECT, City FROM city_table)] -> Det[SEM='SELECT'] N[SEM='City FROM city_table'] *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|[---> . . . .| [0:2] S[SEM=(?np+WHERE+?vp)] -> NP[SEM=?np] * VP[SEM=?vp] {?np: (SELECT, City FROM city_table)}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . [-] . . .| [2:3] IV[SEM=''] -> 'are' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . [-> . . .| [2:3] VP[SEM=(?v+?pp)] -> IV[SEM=?v] * PP[SEM=?pp] {?v: ''}\n",
      "|. . [-> . . .| [2:3] VP[SEM=(?v+?ap)] -> IV[SEM=?v] * AP[SEM=?ap] {?v: ''}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . [-] . .| [3:4] A[SEM=''] -> 'located' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . [-> . .| [3:4] AP[SEM=?pp] -> A[SEM=?a] * PP[SEM=?pp] {?a: ''}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . [-] .| [4:5] P[SEM=''] -> 'in' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . [-> .| [4:5] PP[SEM=(?p+?np)] -> P[SEM=?p] * NP[SEM=?np] {?p: ''}\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . [-]| [5:6] NP[SEM='Country=\"china\"'] -> 'China' *\n",
      "Feature Bottom Up Predict Combine Rule:\n",
      "|. . . . . [->| [5:6] S[SEM=(?np+WHERE+?vp)] -> NP[SEM=?np] * VP[SEM=?vp] {?np: 'Country=\"china\"'}\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. . . . [---]| [4:6] PP[SEM=(, Country=\"china\")] -> P[SEM=''] NP[SEM='Country=\"china\"'] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. . . [-----]| [3:6] AP[SEM=(, Country=\"china\")] -> A[SEM=''] PP[SEM=(, Country=\"china\")] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|. . [-------]| [2:6] VP[SEM=(, , Country=\"china\")] -> IV[SEM=''] AP[SEM=(, Country=\"china\")] *\n",
      "Feature Single Edge Fundamental Rule:\n",
      "|[===========]| [0:6] S[SEM=(SELECT, City FROM city_table, WHERE, , , Country=\"china\")] -> NP[SEM=(SELECT, City FROM city_table)] VP[SEM=(, , Country=\"china\")] *\n",
      "SELECT City FROM city_table WHERE Country=\"china\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import load_parser\n",
    "cp = load_parser('grammars/book_grammars/sql0.fcfg', trace=3)\n",
    "query = 'What cities are located in China'\n",
    "trees = list(cp.parse(query.split()))\n",
    "answer = trees[0].label()['SEM']\n",
    "answer = [s for s in answer if s]\n",
    "q = ' '.join(answer)\n",
    "print(q)\n",
    "\n",
    "# This is from the new version of the book... Go back through the new version as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we execute the query over the database city.db and retrieve some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin\n"
     ]
    }
   ],
   "source": [
    "from nltk.sem import chat80\n",
    "#q = 'SELECT City FROM city_table WHERE Country=\"china\"' # Hardcode in answer from above\n",
    "rows = chat80.sql_query('corpora/city_database/city.db', q)\n",
    "for r in rows: print r[0],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each row r is a one-element tuple, we print out the member of the tuple rather than tuple itself [1].\n",
    "\n",
    "To summarize, we have defined a task where the computer returns useful data in response to a natural language query, and we implemented this by translating a small subset of English into SQL. We can say that our NLTK code already \"understands\" SQL, given that Python is able to execute SQL queries against a database, and by extension it also \"understands\" queries such as What cities are located in China. This parallels being able to translate from Dutch into English as an example of natural language understanding. Suppose that you are a native speaker of English, and have started to learn Dutch. Your teacher asks if you understand what (3) means:\n",
    "\n",
    "(3)\t\tMargrietje houdt van Brunoke.\n",
    "\n",
    "If you know the meanings of the individual words in (3), and know how these meanings are combined to make up the meaning of the whole sentence, you might say that (3) means the same as Margrietje loves Brunoke\n",
    "\n",
    "An observer — let's call her Olga — might well take this as evidence that you do grasp the meaning of (3). But this would depend on Olga herself understanding English. If she doesn't, then your translation from Dutch to English is not going to convince her of your ability to understand Dutch. We will return to this issue shortly.\n",
    "\n",
    "The grammar sql0.fcfg, together with the NLTK Earley parser, is instrumental in carrying out the translation from English to SQL. How adequate is this grammar? You saw that the SQL translation for the whole sentence was built up from the translations of the components. However, there does not seem to be a lot of justification for these component meaning representations. For example, if we look at the analysis of the noun phrase Which cities, the determiner and noun correspond respectively to the SQL fragments SELECT and City FROM city_table. But neither of these have a well-defined meaning in isolation from the other.\n",
    "\n",
    "There is another criticism we can level at the grammar: we have \"hard-wired\" an embarrassing amount of detail about the database into it. We need to know the name of the relevant table (e.g., city_table) and the names of the fields. But our database could have contained exactly the same rows of data yet used a different table name and different field names, in which case the SQL queries would not be executable. Equally, we could have stored our data in a different format, such as XML, in which case retrieving the same results would require us to translate our English queries into an XML query language rather than SQL. These considerations suggest that we should be translating English into something that is more abstract and generic than SQL.\n",
    "\n",
    "In order to sharpen the point, let's consider another English query and its translation:\n",
    "\n",
    "(4)\t\t\n",
    "\n",
    "a.\t\tWhat cities are in China and have populations above 1,000,000?\n",
    "\n",
    "b.\t\tSELECT City FROM city_table WHERE Country = 'china' AND Population > 1000\n",
    "\n",
    "\n",
    "\n",
    "Note\n",
    "\n",
    "Your Turn: Extend the grammar sql0.fcfg so that it will translate (4a) into (4b), and check the values returned by the query.\n",
    "\n",
    "You will probably find it easiest to first extend the grammar to handle queries like What cities have populations above 1,000,000 before tackling conjunction. After you have had a go at this task, you can compare your solution to grammars/book_grammars/sql1.fcfg in the NLTK data distribution.\n",
    "\n",
    "\n",
    "Observe that the and conjunction in (4a) is translated into an AND in the SQL counterpart, (4b). The latter tells us to select results from rows where two conditions are true together: the value of the Country column is 'china' and the value of the Population column is greater than 1000. This interpretation for and involves a new idea: it talks about what is true in some particular situation, and tells us that Cond1 AND Cond2 is true in situation s just in case that condition Cond1 is true in s and condition Cond2 is true in s. Although this doesn't account for the full range of meanings of and in English, it has the nice property that it is independent of any query language. In fact, we have given it the standard interpretation from classical logic. In the following sections, we will explore an approach in which sentences of natural language are translated into logic instead of an executable query language such as SQL. One advantage of logical formalisms is that they are more abstract and therefore more generic. If we wanted to, once we had our translation into logic, we could then translate it into various other special-purpose languages. In fact, most serious attempts to query databases via natural language have used this methodology.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language, Semantics and Logic\n",
    "\n",
    "We started out trying to capture the meaning of (1a) by translating it into a query in another language, SQL, which the computer could interpret and execute. But this still begged the question whether the translation was correct. Stepping back from database query, we noted that the meaning of and seems to depend on being able to specify when statements are true or not in a particular situation. Instead of translating a sentence S from one language to another, we try to say what S is about by relating it to a situation in the world. Let's pursue this further. Imagine there is a situation s where there are two entities, Margrietje and her favourite doll, Brunoke. In addition, there is a relation holding between the two entities, which we will call the love relation. If you understand the meaning of (3), then you know that it is true in situation s. In part, you know this because you know that Margrietje refers to Margrietje, Brunoke refers to Brunoke, and houdt van refers to the love relation.\n",
    "\n",
    "We have introduced two fundamental notions in semantics. The first is that declarative sentences are true or false in certain situations. The second is that definite noun phrases and proper nouns refer to things in the world. So (3) is true in a situation where Margrietje loves the doll Brunoke, here illustrated in 10.1.\n",
    "\n",
    "Once we have adopted the notion of truth in a situation, we have a powerful tool for reasoning. In particular, we can look at sets of sentences, and ask whether they could be true together in some situation. For example, the sentences in (5) can be both true, while those in (6) and (7) cannot be. In other words, the sentences in (5) are consistent, while those in (6) and (7) are inconsistent.\n",
    "\n",
    "(5)\t\t\n",
    "\n",
    "a.\t\tSylvania is to the north of Freedonia.\n",
    "\n",
    "b.\t\tFreedonia is a republic.\n",
    "\n",
    "(6)\t\t\n",
    "\n",
    "a.\t\tThe capital of Freedonia has a population of 9,000.\n",
    "\n",
    "b.\t\tNo city in Freedonia has a population of 9,000.\n",
    "\n",
    "(7)\t\t\n",
    "\n",
    "a.\t\tSylvania is to the north of Freedonia.\n",
    "\n",
    "b.\t\tFreedonia is to the north of Sylvania.\n",
    "\n",
    "We have chosen sentences about fictional countries (featured in the Marx Brothers' 1933 movie Duck Soup) to emphasize that your ability to reason about these examples does not depend on what is true or false in the actual world. If you know the meaning of the word no, and also know that the capital of a country is a city in that country, then you should be able to conclude that the two sentences in (6) are inconsistent, regardless of where Freedonia is or what the population of its capital is. That is, there's no possible situation in which both sentences could be true. Similarly, if you know that the relation expressed by to the north of is asymmetric, then you should be able to conclude that the two sentences in (7) are inconsistent.\n",
    "\n",
    "Broadly speaking, logic-based approaches to natural language semantics focus on those aspects of natural language which guide our judgments of consistency and inconsistency. The syntax of a logical language is designed to make these features formally explicit. As a result, determining properties like consistency can often be reduced to symbolic manipulation, that is, to a task that can be carried out by a computer. In order to pursue this approach, we first want to develop a technique for representing a possible situation. We do this in terms of something that logicians call a model.\n",
    "\n",
    "A model for a set W of sentences is a formal representation of a situation in which all the sentences in W are true. The usual way of representing models involves set theory. The domain D of discourse (all the entities we currently care about) is a set of individuals, while relations are treated as sets built up from D. Let's look at a concrete example. Our domain D will consist of three children, Stefan, Klaus and Evi, represented respectively as s, k and e. We write this as D = {s, k, e}. The expression boy denotes the set consisting of Stefan and Klaus, the expression girl denotes the set consisting of Evi, and the expression is running denotes the set consisting of Stefan and Evi. 10.2 is a graphical rendering of the model.\n",
    "\n",
    "Later in this chapter we will use models to help evaluate the truth or falsity of English sentences, and in this way to illustrate some methods for representing meaning. However, before going into more detail, let's put the discussion into a broader perspective, and link back to a topic that we briefly raised in 1.5. Can a computer understand the meaning of a sentence? And how could we tell if it did? This is similar to asking \"Can a computer think?\" Alan Turing famously proposed to answer this by examining the ability of a computer to hold sensible conversations with a human (Turing, 1950). Suppose you are having a chat session with a person and a computer, but you are not told at the outset which is which. If you cannot identify which of your partners is the computer after chatting with each of them, then the computer has successfully imitated a human. If a computer succeeds in passing itself off as human in this \"imitation game\" (or \"Turing Test\" as it is popularly known), then according to Turing, we should be prepared to say that the computer can think and can be said to be intelligent. So Turing side-stepped the question of somehow examining the internal states of a computer by instead using its behavior as evidence of intelligence. By the same reasoning, we have assumed that in order to say that a computer understands English, it just needs to behave as though it did. What is important here is not so much the specifics of Turing's imitation game, but rather the proposal to judge a capacity for natural language understanding in terms of observable behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2   Propositional Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A logical language is designed to make reasoning formally explicit. As a result, it can capture aspects of natural language which determine whether a set of sentences is consistent. As part of this approach, we need to develop logical representations of a sentence φ which formally capture the truth-conditions of φ. We'll start off with a simple example:\n",
    "\n",
    "(8)\t\t[Klaus chased Evi] and [Evi ran away].\n",
    "\n",
    "Let's replace the two sub-sentences in (8) by φ and ψ respectively, and put & for the logical operator corresponding to the English word and: φ & ψ. This structure is the logical form of (8).\n",
    "\n",
    "Propositional logic allows us to represent just those parts of linguistic structure which correspond to certain sentential connectives. We have just looked at and. Other such connectives are not, or and if..., then.... In the formalization of propositional logic, the counterparts of such connectives are sometimes called boolean operators. The basic expressions of propositional logic are propositional symbols, often written as P, Q, R, etc. There are varying conventions for representing boolean operators. Since we will be focusing on ways of exploring logic within NLTK, we will stick to the following ASCII versions of the operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negation       \t-\n",
      "conjunction    \t&\n",
      "disjunction    \t|\n",
      "implication    \t->\n",
      "equivalence    \t<->\n"
     ]
    }
   ],
   "source": [
    "nltk.boolean_ops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the propositional symbols and the boolean operators we can build an infinite set of well formed formulas (or just formulas, for short) of propositional logic. First, every propositional letter is a formula. Then if φ is a formula, so is -φ. And if φ and ψ are formulas, then so are (φ & ψ) (φ | ψ) (φ -> ψ) (φ <-> ψ).\n",
    "\n",
    "The Table 2.1 specifies the truth-conditions for formulas containing these operators. As before we use φ and ψ as variables over sentences, and abbreviate if and only if as iff.\n",
    "\n",
    "Table 2.1:\n",
    "\n",
    "Truth conditions for the Boolean Operators in Propositional Logic.\n",
    "[skipped table]\n",
    "\n",
    "These rules are generally straightforward, though the truth conditions for implication departs in many cases from our usual intuitions about the conditional in English. A formula of the form (P -> Q) is only false when P is true and Q is false. If P is false (say P corresponds to The moon is made of green cheese) and Q is true (say Q corresponds to Two plus two equals four) then P -> Q will come out true.\n",
    "\n",
    "NLTKs LogicParser() parses logical expressions into various subclasses of Expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NegatedExpression -(P & Q)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "read_expr('-(P & Q)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AndExpression (P & Q)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('P & Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrExpression (P | (R -> Q))>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('P | (R -> Q)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IffExpression (P <-> --P)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('P <-> -- P')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "From a computational perspective, logics give us an important tool for performing inference. Suppose you state that Freedonia is not to the north of Sylvania, and you give as your reasons that Sylvania is to the north of Freedonia. In this case, you have produced an argument. The sentence Sylvania is to the north of Freedonia is the assumption of the argument while Freedonia is not to the north of Sylvania is the conclusion. The step of moving from one or more assumptions to a conclusion is called inference. Informally, it is common to write arguments in a format where the conclusion is preceded by therefore.\n",
    "\n",
    "(9)\t\t\n",
    "Sylvania is to the north of Freedonia.\n",
    "Therefore, Freedonia is not to the north of Sylvania\n",
    "\n",
    "An argument is valid if there is no possible situation in which its premises are all true and its conclusion is not true.\n",
    "\n",
    "Now, the validity of (9) crucially depends on the meaning of the phrase to the north of, in particular, the fact that it is an asymmetric relation:\n",
    "\n",
    "(10)\t\tif x is to the north of y then y is not to the north of x.\n",
    "\n",
    "Unfortunately, we can't express such rules in propositional logic: the smallest elements we have to play with are atomic propositions, and we cannot \"look inside\" these to talk about relations between individuals x and y. The best we can do in this case is capture a particular case of the asymmetry. Let's use the propositional symbol SnF to stand for Sylvania is to the north of Freedonia and FnS for Freedonia is to the north of Sylvania. To say that Freedonia is not to the north of Sylvania, we write -FnS That is, we treat not as equivalent to the phrase it is not the case that ..., and translate this as the one-place boolean operator -. So now we can write the implication in (10) as\n",
    "\n",
    "(11)\t\tSnF -> -FnS\n",
    "\n",
    "How about giving a version of the complete argument? We will replace the first sentence of (9) by two formulas of propositional logic: SnF, and also the implication in (11), which expresses (rather poorly) our background knowledge of the meaning of to the north of. We'll write [A1, ..., An] / C to represent the argument that conclusion C follows from assumptions [A1, ...,\n",
    "An]. This leads to the following as a representation of argument (9):\n",
    "\n",
    "(12)\t\t[SnF, SnF -> -FnS] / -FnS\n",
    "\n",
    "This is a valid argument: if SnF and SnF -> -FnS are both true in a situation s, then -FnS must also be true in s. By contrast, if FnS were true, this would conflict with our understanding that two objects cannot both be to the north of each other in any possible situation. Equivalently, the list [SnF, SnF -> -FnS, FnS] is inconsistent — these sentences cannot all be true together.\n",
    "\n",
    "Arguments can be tested for \"syntactic validity\" by using a proof system. We will say a little bit more about this later on in 3. Logical proofs can be carried out with NLTK's inference module, for example via an interface to the third-party theorem prover Prover9. The inputs to the inference mechanism first have to be converted into logical expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the prover9 file!\nUse software specific configuration paramaters or set the PROVER9 environment variable.\n\n  Searched in:\n    - /usr/local/bin/prover9\n    - /usr/local/bin/prover9/bin\n    - /usr/local/bin\n    - /usr/bin\n    - /usr/local/prover9\n    - /usr/local/share/prover9\n\n  For more information on prover9, see:\n    <http://www.cs.unm.edu/~mccune/prover9/>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2385bd58af8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprover\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProver9\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# convenient way to call NLTK's Prover9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[1;31m#prover.config_prover9('C:/dropbox/hp_m6/cgraph/prover9.py')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprover\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNotFnS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSnF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# See if NotFnS is the conclusion of the arguments SnF and R\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\api.pyc\u001b[0m in \u001b[0;36mprove\u001b[0;34m(self, goal, assumptions, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massumptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massumptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_prove\u001b[0;34m(self, goal, assumptions, verbose)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         stdout, returncode = self._call_prover9(self.prover9_input(goal, assumptions),\n\u001b[0;32m--> 272\u001b[0;31m                                                 verbose=verbose)\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_call_prover9\u001b[0;34m(self, input_str, args, verbose)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prover9_bin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prover9_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prover9'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mupdated_input_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_find_binary\u001b[0;34m(self, name, verbose)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://www.cs.unm.edu/~mccune/prover9/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.exe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 binary_names=None, url=None, verbose=True):\n\u001b[1;32m    601\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 602\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \"\"\"\n\u001b[1;32m    595\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 596\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    565\u001b[0m                         (filename, url))\n\u001b[1;32m    566\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the prover9 file!\nUse software specific configuration paramaters or set the PROVER9 environment variable.\n\n  Searched in:\n    - /usr/local/bin/prover9\n    - /usr/local/bin/prover9/bin\n    - /usr/local/bin\n    - /usr/bin\n    - /usr/local/prover9\n    - /usr/local/share/prover9\n\n  For more information on prover9, see:\n    <http://www.cs.unm.edu/~mccune/prover9/>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# read_expr = nltk.sem.Expression.fromstring\n",
    "lp = nltk.sem.Expression.fromstring\n",
    "SnF = read_expr('SnF') # SnF means Sylvania is to the north of Fredonia\n",
    "NotFnS = read_expr('-FnS') # NotFnS means Fredonia is not to the north of Sylvania\n",
    "R = read_expr('SnF -> -FnS') # R stands for relation?\n",
    "\n",
    "# import sys\n",
    "# sys.path.insert(0, 'C:/Users/bwbel/Anaconda2/Lib/site-packages/nltk/inference/prover9.py')\n",
    "# import prover9\n",
    "\n",
    "prover = nltk.Prover9() # convenient way to call NLTK's Prover9\n",
    "#prover.config_prover9('C:/dropbox/hp_m6/cgraph/prover9.py')\n",
    "prover.prove(NotFnS, [SnF, R]) # See if NotFnS is the conclusion of the arguments SnF and R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another way of seeing why the conclusion follows. SnF -> -FnS is semantically equivalent to -SnF | -FnS, where \"|\" is the two-place operator corresponding to or. In general, φ | ψ is true in a situation s if either φ is true in s or φ is true in s. Now, suppose both SnF and -SnF | -FnS are true in situation s. If SnF is true, then -SnF cannot also be true; a fundamental assumption of classical logic is that a sentence cannot be both true and false in a situation. Consequently, -FnS must be true.\n",
    "\n",
    "Recall that we interpret sentences of a logical language relative to a model, which is a very simplified version of the world. A model for propositional logic needs to assign the values True or False to every possible formula. We do this inductively: first, every propositional symbol is assigned a value, and then we compute the value of complex formulas by consulting the meanings of the boolean operators (i.e, 2.1) and applying them to the values of the formula's components. A Valuation is a mapping from basic expressions of the logic to their values. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = nltk.Valuation([('P', True), ('Q', True), ('R', False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a Valuation with a list of pairs, each of which consists of a semantic symbol and a semantic value. The resulting object is essentially just a dictionary that maps logical expressions (treated as strings) to appropriate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['P']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will see later, our models need to be somewhat more complicated in order to handle the more complex logical forms discussed in the next section; for the time being, just ignore the dom and g parameters in the following declarations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dom = set()\n",
    "g = nltk.Assignment(dom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize a model m that uses val:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = nltk.Model(dom, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every model comes with an evaluate() method, which will determine the semantic value of logical expressions, such as formulas of propositional logic; of course, these values depend on the initial truth values we assigned to propositional symbols such as P, Q and R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(m.evaluate('(P & Q)', g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(m.evaluate('-(P & Q)', g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(m.evaluate('(P & R)', g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(m.evaluate('(P | R)', g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, we have been translating our English sentences into propositional logic. Because we are confined to representing atomic sentences with letters like P and Q, we cannot dig into their internal structure. In effect, we are saying that there is nothing of logical interest to dividing atomic sentences into subjects, objects and predicates. However, this seems wrong: if we want to formalize arguments such as (9), we have to be able to \"look inside\" basic sentences. As a result, we will move beyond Propositional Logic to a something more expressive, namely First-Order Logic. This is what we turn to in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3 First-Order Logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the remainder of this chapter, we will represent the meaning of natural language expressions by translating them into first-order logic. Not all of natural language semantics can be expressed in first-order logic. But it is a good choice for computational semantics because it is expressive enough to represent a good deal, and on the other hand, there are excellent systems available off the shelf for carrying out automated inference in first order logic.\n",
    "\n",
    "Our next step will be to describe how formulas of first-order logic are constructed, and then how such formulas can be evaluated in a model.\n",
    "\n",
    "## 3.1   Syntax\n",
    "\n",
    "First-order logic keeps all the boolean operators of Propositional Logic. But it adds some important new mechanisms. To start with, propositions are analyzed into predicates and arguments, which takes us a step closer to the structure of natural languages. The standard construction rules for first-order logic recognize terms such as individual variables and individual constants, and predicates which take differing numbers of arguments. For example, Angus walks might be formalized as walk(angus) and Angus sees Bertie as see(angus, bertie). We will call walk a unary predicate, and see a binary predicate. The symbols used as predicates do not have intrinsic meaning, although it is hard to remember this. Returning to one of our earlier examples, there is no logical difference between (13a) and (13b).\n",
    "\n",
    "(13)\t\t\n",
    "\n",
    "a.\t\tlove(margrietje, brunoke)\n",
    "\n",
    "b.\t\thouden_van(margrietje, brunoke)\n",
    "\n",
    "By itself, first-order logic has nothing substantive to say about lexical semantics — the meaning of individual words — although some theories of lexical semantics can be encoded in first-order logic. Whether an atomic predication like see(angus, bertie) is true or false in a situation is not a matter of logic, but depends on the particular valuation that we have chosen for the constants see, angus and bertie. For this reason, such expressions are called non-logical constants. By contrast, logical constants (such as the boolean operators) always receive the same interpretation in every model for first-order logic.\n",
    "\n",
    "We should mention here that one binary predicate has special status, namely equality, as in formulas such as angus = aj. Equality is regarded as a logical constant, since for individual terms t1 and t2, the formula t1 = t2 is true if and only if t1 and t2 refer to one and the same entity.\n",
    "\n",
    "It is often helpful to inspect the syntactic structure of expressions of first-order logic, and the usual way of doing this is to assign types to expressions. Following the tradition of Montague grammar, we will use two basic types: e is the type of entities, while t is the type of formulas, i.e., expressions which have truth values. Given these two basic types, we can form complex types for function expressions. That is, given any types σ and τ, 〈σ, τ〉 is a complex type corresponding to functions from 'σ things' to 'τ things'. For example, 〈e, t〉 is the type of expressions from entities to truth values, namely unary predicates. The logical expression can be processed with type checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConstantExpression angus>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "expr = read_expr('walk(angus)', type_check=True)\n",
    "expr.argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr.argument.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConstantExpression walk>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr.function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<e,?>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr.function.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Why do we see <e,?> at the end of this example? Although the type-checker will try to infer as many types as possible, in this case it has not managed to fully specify the type of walk, since its result type is unknown. Although we are intending walk to receive type <e, t>, as far as the type-checker knows, in this context it could be of some other type such as <e, e> or <e, <e, t>. To help the type-checker, we need to specify a signature, implemented as a dictionary that explicitly associates types with non-logical constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "e"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig = {'walk': '<e, t>'}\n",
    "expr = read_expr('walk(angus)', signature=sig)\n",
    "expr.function.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A binary predicate has type 〈e, 〈e, t〉〉. Although this is the type of something which combines first with an argument of type e to make a unary predicate, we represent binary predicates as combining directly with their two arguments. For example, the predicate see in the translation of Angus sees Cyril will combine with its arguments to give the result see(angus, cyril).\n",
    "\n",
    "In first-order logic, arguments of predicates can also be individual variables such as x, y and z. In NLTK, we adopt the convention that variables of type e are all lowercase. Individual variables are similar to personal pronouns like he, she and it, in that we need to know about the context of use in order to figure out their denotation.\n",
    "\n",
    "One way of interpreting the pronoun in (14) is by pointing to a relevant individual in the local context.\n",
    "\n",
    "(14)\t\tHe disappeared.\n",
    "\n",
    "Another way is to supply a textual antecedent for the pronoun he, for example by uttering (15a) prior to (14). Here, we say that he is coreferential with the noun phrase Cyril. As a result, (14) is semantically equivalent to (15b).\n",
    "\n",
    "(15)\t\t\n",
    "\n",
    "a.\t\tCyril is Angus's dog.\n",
    "\n",
    "b.\t\tCyril disappeared.\n",
    "\n",
    "Consider by contrast the occurrence of he in (16a). In this case, it is bound by the indefinite NP a dog, and this is a different relationship than coreference. If we replace the pronoun he by a dog, the result (16b) is not semantically equivalent to (16a).\n",
    "\n",
    "(16)\t\t\n",
    "\n",
    "a.\t\tAngus had a dog but he disappeared.\n",
    "\n",
    "b.\t\tAngus had a dog but a dog disappeared.\n",
    "\n",
    "Corresponding to (17a), we can construct an open formula (17b) with two occurrences of the variable x. (We ignore tense to simplify exposition.)\n",
    "\n",
    "(17)\t\t\n",
    "\n",
    "a.\t\tHe is a dog and he disappeared.\n",
    "\n",
    "b.\t\tdog(x) ∧ disappear(x)\n",
    "\n",
    "By placing an existential quantifier ∃x ('for some x') in front of (17b), we can bind these variables, as in (18a), which means (18b) or, more idiomatically, (18c).\n",
    "\n",
    "(18)\t\t\n",
    "\n",
    "a.\t\t∃x.(dog(x) ∧ disappear(x))\n",
    "\n",
    "b.\t\tAt least one entity is a dog and disappeared.\n",
    "\n",
    "c.\t\tA dog disappeared.\n",
    "\n",
    "The NLTK rendering of (18a):\n",
    "\n",
    "(19)\t\texists x.(dog(x) & disappear(x))\n",
    "\n",
    "In addition to the existential quantifier, first-order logic offers us the universal quantifier ∀x ('for all x'), illustrated in (20).\n",
    "\n",
    "(20)\t\t\n",
    "\n",
    "a.\t\t∀x.(dog(x) → disappear(x))\n",
    "\n",
    "b.\t\tEverything has the property that if it is a dog, it disappears.\n",
    "\n",
    "c.\t\tEvery dog disappeared.\n",
    "\n",
    "The NLTK syntax for (20a):\n",
    "\n",
    "(21)\t\tall x.(dog(x) -> disappear(x))\n",
    "\n",
    "Although (20a) is the standard first-order logic translation of (20c), the truth conditions aren't necessarily what you expect. The formula says that if some x is a dog, then x disappears — but it doesn't say that there are any dogs. So in a situation where there are no dogs, (20a) will still come out true. (Remember that (P -> Q) is true when P is false.) Now you might argue that every dog disappeared does presuppose the existence of dogs, and that the logic formalization is simply wrong. But it is possible to find other examples which lack such a presupposition. For instance, we might explain that the value of the Python expression astring.replace('ate', '8') is the result of replacing every occurrence of 'ate' in astring by '8', even though there may in fact be no such occurrences (3.2).\n",
    "\n",
    "We have seen a number of examples where variables are bound by quantifiers. What happens in formulas such as the following?:\n",
    "\n",
    "((exists x. dog(x)) -> bark(x))\n",
    "\n",
    "The scope of the exists x quantifier is dog(x), so the occurrence of x in bark(x) is unbound. Consequently it can become bound by some other quantifier, for example all x in the next formula:\n",
    "\n",
    "all x.((exists x. dog(x)) -> bark(x))\n",
    "\n",
    "In general, an occurrence of a variable x in a formula φ is free in φ if that occurrence doesn't fall within the scope of all x or some x in φ. Conversely, if x is free in formula φ, then it is bound in all x.φ and exists x.φ. If all variable occurrences in a formula are bound, the formula is said to be closed.\n",
    "\n",
    "We mentioned before that the Expression object can process strings, and returns objects of class Expression. Each instance expr of this class comes with a method free() which returns the set of variables that are free in expr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr = nltk.sem.Expression.fromstring\n",
    "read_expr('dog(cyril)').free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Variable('x')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('dog(x)').free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('own(angus, cyril)').free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('exists x.dog(x)').free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Variable('x')}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('((some x. walk(x)) -> sing(x))').free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Variable('y')}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_expr('exists x.own(y, x)').free()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't presently understand this section on first-order logic. come back to this if it is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3.2   First Order Theorem Proving\n",
    "\n",
    "Recall the constraint on to the north of which we proposed earlier as (10):\n",
    "\n",
    "(22)\t\tif x is to the north of y then y is not to the north of x.\n",
    "\n",
    "We observed that propositional logic is not expressive enough to represent generalizations about binary predicates, and as a result we did not properly capture the argument Sylvania is to the north of Freedonia. Therefore, Freedonia is not to the north of Sylvania.\n",
    "\n",
    "You have no doubt realized that first order logic, by contrast, is ideal for formalizing such rules:\n",
    "\n",
    "all x. all y.(north_of(x, y) -> -north_of(y, x))\n",
    "\n",
    "Even better, we can perform automated inference to show the validity of the argument.\n",
    "\n",
    "The general case in theorem proving is to determine whether a formula that we want to prove (a proof goal) can be derived by a finite sequence of inference steps from a list of assumed formulas. We write this as S ⊢ g, where S is a (possibly empty) list of assumptions, and g is a proof goal. We will illustrate this with NLTK's interface to the theorem prover Prover9. First, we parse the required proof goal [1] and the two assumptions [2] [3]. Then we create a Prover9 instance [4], and call its prove() method on the goal, given the list of assumptions [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the prover9 file!\nUse software specific configuration paramaters or set the PROVER9 environment variable.\n\n  Searched in:\n    - /usr/local/bin/prover9\n    - /usr/local/bin/prover9/bin\n    - /usr/local/bin\n    - /usr/bin\n    - /usr/local/prover9\n    - /usr/local/share/prover9\n\n  For more information on prover9, see:\n    <http://www.cs.unm.edu/~mccune/prover9/>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-d157e5f0fa90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_expr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'all x. all y. (north_of(x, y) -> -north_of(y, x))'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprover\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mProver9\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprover\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNotFnS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSnF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\api.pyc\u001b[0m in \u001b[0;36mprove\u001b[0;34m(self, goal, assumptions, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massumptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massumptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_prove\u001b[0;34m(self, goal, assumptions, verbose)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         stdout, returncode = self._call_prover9(self.prover9_input(goal, assumptions),\n\u001b[0;32m--> 272\u001b[0;31m                                                 verbose=verbose)\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_call_prover9\u001b[0;34m(self, input_str, args, verbose)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prover9_bin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prover9_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prover9'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mupdated_input_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_find_binary\u001b[0;34m(self, name, verbose)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://www.cs.unm.edu/~mccune/prover9/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.exe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 binary_names=None, url=None, verbose=True):\n\u001b[1;32m    601\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 602\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \"\"\"\n\u001b[1;32m    595\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 596\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    565\u001b[0m                         (filename, url))\n\u001b[1;32m    566\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the prover9 file!\nUse software specific configuration paramaters or set the PROVER9 environment variable.\n\n  Searched in:\n    - /usr/local/bin/prover9\n    - /usr/local/bin/prover9/bin\n    - /usr/local/bin\n    - /usr/bin\n    - /usr/local/prover9\n    - /usr/local/share/prover9\n\n  For more information on prover9, see:\n    <http://www.cs.unm.edu/~mccune/prover9/>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "NotFnS = read_expr('-north_of(f, s)')  \n",
    "SnF = read_expr('north_of(s, f)')  \n",
    "R = read_expr('all x. all y. (north_of(x, y) -> -north_of(y, x))')\n",
    "prover = nltk.Prover9()   \n",
    "prover.prove(NotFnS, [SnF, R])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happily, the theorem prover agrees with us that the argument is valid. By contrast, it concludes that it is not possible to infer north_of(f, s) from our assumptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n\n===========================================================================\nNLTK was unable to find the prover9 file!\nUse software specific configuration paramaters or set the PROVER9 environment variable.\n\n  Searched in:\n    - /usr/local/bin/prover9\n    - /usr/local/bin/prover9/bin\n    - /usr/local/bin\n    - /usr/bin\n    - /usr/local/prover9\n    - /usr/local/share/prover9\n\n  For more information on prover9, see:\n    <http://www.cs.unm.edu/~mccune/prover9/>\n===========================================================================",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b3d1d6ad6eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mFnS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_expr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'north_of(f, s)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprover\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFnS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mSnF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\api.pyc\u001b[0m in \u001b[0;36mprove\u001b[0;34m(self, goal, assumptions, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \"\"\"\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgoal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massumptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massumptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_prove\u001b[0;34m(self, goal, assumptions, verbose)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         stdout, returncode = self._call_prover9(self.prover9_input(goal, assumptions),\n\u001b[0;32m--> 272\u001b[0;31m                                                 verbose=verbose)\n\u001b[0m\u001b[1;32m    273\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_call_prover9\u001b[0;34m(self, input_str, args, verbose)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prover9_bin\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prover9_bin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'prover9'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mupdated_input_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\inference\\prover9.pyc\u001b[0m in \u001b[0;36m_find_binary\u001b[0;34m(self, name, verbose)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'http://www.cs.unm.edu/~mccune/prover9/'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mbinary_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.exe'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    600\u001b[0m                 binary_names=None, url=None, verbose=True):\n\u001b[1;32m    601\u001b[0m     return next(find_binary_iter(name, path_to_bin, env_vars, searchpath,\n\u001b[0;32m--> 602\u001b[0;31m                                  binary_names, url, verbose))\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m def find_jar_iter(name_pattern, path_to_jar=None, env_vars=(),\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[0;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \"\"\"\n\u001b[1;32m    595\u001b[0m     for file in  find_file_iter(path_to_bin or name, env_vars, searchpath, binary_names,\n\u001b[0;32m--> 596\u001b[0;31m                      url, verbose):\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\bwbel\\Anaconda2\\lib\\site-packages\\nltk\\__init__.pyc\u001b[0m in \u001b[0;36mfind_file_iter\u001b[0;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[1;32m    565\u001b[0m                         (filename, url))\n\u001b[1;32m    566\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the prover9 file!\nUse software specific configuration paramaters or set the PROVER9 environment variable.\n\n  Searched in:\n    - /usr/local/bin/prover9\n    - /usr/local/bin/prover9/bin\n    - /usr/local/bin\n    - /usr/bin\n    - /usr/local/prover9\n    - /usr/local/share/prover9\n\n  For more information on prover9, see:\n    <http://www.cs.unm.edu/~mccune/prover9/>\n==========================================================================="
     ]
    }
   ],
   "source": [
    "FnS = read_expr('north_of(f, s)')\n",
    "prover.prove(FnS, [SnF, R])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3   Summarizing the Language of First Order Logic\n",
    "\n",
    "We'll take this opportunity to restate our earlier syntactic rules for propositional logic and add the formation rules for quantifiers; together, these give us the syntax of first order logic. In addition, we make explicit the types of the expressions involved. We'll adopt the convention that 〈en, t〉 is the type of a predicate which combines with n arguments of type e to yield an expression of type t. In this case, we say that n is the arity of the predicate.\n",
    "\n",
    "        If P is a predicate of type 〈en, t〉, and α1, ... αn are terms of type e, then P(α1, ... αn) is of type t.\n",
    "        If α and β are both of type e, then (α = β) and (α != β) are of type t.\n",
    "        If φ is of type t, then so is -φ.\n",
    "        If φ and ψ are of type t, then so are (φ & ψ), (φ | ψ), (φ -> ψ) and (φ <-> ψ).\n",
    "        If φ is of type t, and x is a variable of type e, then exists x.φ and all x.φ are of type t.\n",
    "\n",
    "3.1 summarizes the new logical constants of the logic module, and two of the methods of Expressions.\n",
    "\n",
    "Table 3.1:\n",
    "\n",
    "Summary of new logical relations and operators required for First Order Logic, together with two useful methods of the Expression class.\n",
    "\n",
    "Example \tDescription\n",
    "= \tequality\n",
    "!= \tinequality\n",
    "exists \texistential quantifier\n",
    "all \tuniversal quantifier\n",
    "e.free() \tshow free variables of e\n",
    "e.simplify() \tcarry out β-reduction on e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4   Truth in Model\n",
    "\n",
    "We have looked at the syntax of first-order logic, and in 4 we will examine the task of translating English into first-order logic. Yet as we argued in 1, this only gets us further forward if we can give a meaning to sentences of first-order logic. In other words, we need to give a truth-conditional semantics to first-order logic. From the point of view of computational semantics, there are obvious limits in how far one can push this approach. Although we want to talk about sentences being true or false in situations, we only have the means of representing situations in the computer in a symbolic manner. Despite this limitation, it is still possible to gain a clearer picture of truth-conditional semantics by encoding models in NLTK.\n",
    "\n",
    "Given a first-order logic language L, a model M for L is a pair 〈D, Val〉, where D is an nonempty set called the domain of the model, and Val is a function called the valuation function which assigns values from D to expressions of L as follows:\n",
    "\n",
    "        For every individual constant c in L, Val(c) is an element of D.\n",
    "        For every predicate symbol P of arity n ≥ 0, Val(P) is a function from Dn to {True, False}. (If the arity of P is 0, then Val(P) is simply a truth value, the P is regarded as a propositional symbol.)\n",
    "\n",
    "According to (ii), if P is of arity 2, then Val(P) will be a function f from pairs of elements of D to {True, False}. In the models we shall build in NLTK, we'll adopt a more convenient alternative, in which Val(P) is a set S of pairs, defined as follows:\n",
    "\n",
    "(23)\t\tS = {s | f(s) = True}\n",
    "\n",
    "Such an f is called the characteristic function of S (as discussed in the further readings).\n",
    "\n",
    "Relations are represented semantically in NLTK in the standard set-theoretic way: as sets of tuples. For example, let's suppose we have a domain of discourse consisting of the individuals Bertie, Olive and Cyril, where Bertie is a boy, Olive is a girl and Cyril is a dog. For mnemonic reasons, we use b, o and c as the corresponding labels in the model. We can declare the domain as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dom = {'b', 'o', 'c'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the utility function Valuation.fromstring() to convert a list of strings of the form symbol => value into a Valuation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boy': set([('b',)]), 'cyril': 'c', 'dog': set([('c',)]), 'walk': set([('c',), ('o',)]), 'bertie': 'b', 'see': set([('o', 'c'), ('c', 'b'), ('b', 'o')]), 'olive': 'o', 'girl': set([('o',)])}\n"
     ]
    }
   ],
   "source": [
    "v = \"\"\"\n",
    "    bertie => b\n",
    "    olive => o\n",
    "    cyril => c\n",
    "    boy => {b}\n",
    "    girl => {o}\n",
    "    dog => {c}\n",
    "    walk => {o, c}\n",
    "    see => {(b, o), (c, b), (o, c)}\n",
    "    \"\"\"\n",
    "val = nltk.Valuation.fromstring(v)\n",
    "print(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So according to this valuation, the value of see is a set of tuples such that Bertie sees Olive, Cyril sees Bertie, and Olive sees Cyril."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that our unary predicates (i.e, boy, girl, dog) also come out as sets of singleton tuples, rather than just sets of individuals. This is a convenience which allows us to have a uniform treatment of relations of any arity. A predication of the form P(τ1, ... τn), where P is of arity n, comes out true just in case the tuple of values corresponding to (τ1, ... τn) belongs to the set of tuples in the value of P."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('o', 'c') in val['see']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('b',) in val['boy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.5   Individual Variables and Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our models, the counterpart of a context of use is a variable assignment. This is a mapping from individual variables to entities in the domain. Assignments are created using the Assignment constructor, which also takes the model's domain of discourse as a parameter. We are not required to actually enter any bindings, but if we do, they are in a (variable, value) format similar to what we saw earlier for valuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 'c', 'x': 'o'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = nltk.Assignment(dom, [('x', 'o'), ('y', 'c')])\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
